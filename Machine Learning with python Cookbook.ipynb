{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9107a4",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING WITH PYTHON COOKBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01f6e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------- # Chapter1: Vectors, Matrices, Arrays # --------------------------------\n",
    "# Create vector as a row\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "vector_row = np.array([1,2,3])\n",
    "vector_column = np.array([[1],[2],[3]])\n",
    "matrix = np.array([[1,2],\n",
    "                 [1,2],\n",
    "                 [1,2]])\n",
    "# Create Sparse Matrix\n",
    "matrix_sparse = sparse.csr_matrix(np.array([[0,0,1],[0,1,0],[5,3,0]]))\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "#Apply Operations to Elements\n",
    "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "vectorized_function = np.vectorize(lambda x: x+100)\n",
    "vectorized_function(matrix)\n",
    "# Finding Max&Min\n",
    "np.max(matrix, axis=0)\n",
    "np.min(matrix, axis=1)\n",
    "# Calculate Average, Variance, Standard Deviation\n",
    "np.mean(matrix) # axis option\n",
    "np.var(matrix)\n",
    "np.std(matrix)\n",
    "# Reshape Arrays\n",
    "matrix.reshape(-1,1) #reshape(1,-1)\n",
    "# Transposing Matrix\n",
    "matrix.T\n",
    "# Flattening a Matrix (1-dimensional array)\n",
    "matrix.flatten()\n",
    "# Finding Rank of Matrix\n",
    "np.linalg.matrix_rank(matrix)\n",
    "# Calculate the Determinant\n",
    "np.linalg.det(matrix)\n",
    "# Getting the Diagonal of a Matrix\n",
    "matrix.diagonal(offset=0)\n",
    "# Calculate Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "# Inverting a Matrix\n",
    "np.linalg.inv(matrix)\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Random Values\n",
    "np.random.seed(0)\n",
    "np.random.random(3)\n",
    "np.random.randint(0,11,3)\n",
    "# Draw numbers from a normal distribution with mean=0 and std = 1\n",
    "np.random.normal(0.0,1.0,5)\n",
    "# Draw numbers from a logistic distribution with mean=0 and scale of 1.0\n",
    "np.random.logistic(0.0,1.0,5)\n",
    "# Draw numbers greater than or equal to 1.0 and less than 2.0\n",
    "np.random.uniform(1.0,2.0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01a9f720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t5\n",
      "  (2, 1)\t3\n"
     ]
    }
   ],
   "source": [
    "print(matrix_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------# Chapter2: Loading Data #---------------------------------\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "target = digits.target\n",
    "\"\"\"toy dataset: load_boston, load_iris, load_digits\"\"\"\n",
    "#Load data from SQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "database_connection = create_engine('sqlite://sample.db')\n",
    "df = pd.read_sql_query('SELECT * FROM data',database_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ea447",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------# Chapter3: Data Wrangling ------------------------------\n",
    "# Replace values with regular expressions:\n",
    "df.replace(r\"[0-9]\",\"\", regex=True)\n",
    "# Grouping rows by Time\n",
    "time_index = pd.date_range('06/06/2017', periods=100000, freq='305')\n",
    "df = pd.DataFrame(index=time_index)\n",
    "# Group rows by week\n",
    "df.resample('W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------# Chapter4: Handling Numerical Data ------------------------------------------------------\n",
    "from sklearn import preprocessing\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_feture = minmax_scale.fit_transform(feature)\n",
    "robust_scaler = preprocessing.RobustScaler() # outliers standard\n",
    "# Normalizing Observations\n",
    "\"\"\"Normalizer rescales the values on individual observations \"\"\"\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "# PolynomialFeatures\n",
    "interaction = PolynomialFeatures(degree=2)\n",
    "# Transforming Features\n",
    "\"Like pandas apply\"\n",
    "feature_transform = FunctionTransformer(function) \n",
    "\n",
    "# Detect outliers\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "def indices_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25,75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    uper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "\n",
    "# Imputing missing values\n",
    "\"\"\"missing value using KNN\"\"\"\n",
    "features_knn_imputed = KNN(k-5, verbose=0).complete(standardized_features)\n",
    "\"\"\"missing value using Imputer\"\"\"\n",
    "mean_imputer = Imputer(strategy=\"mean\", axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------- # Chapter5: Handling Categorical Data -----------------------------------------\n",
    "# Encoding nominal categorical features\n",
    "from sklearn.processing import LabelBinarizer, MultiLabelBinarizer\n",
    "one_hot = LabelBinarizer()\n",
    "one_hot.fit_transform(feature)\n",
    "\"\"\" reverse one-hot encoding\"\"\"\n",
    "one_hot.reverse_transform(one_hot.fit_transform(feature))\n",
    "\"\"\" using pandas get_dummies\"\"\"\n",
    "pd.get_dummies(feature[:,0])\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "--------------------------------------------------------------------------------------\n",
    "# Imputing Missing Class Values\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassfier\n",
    "X = np.array([[0, 2.1, 1.45],\n",
    "             [1, 1.18, 1.33],\n",
    "             [0, 1.22, 1.27],\n",
    "             [1, -0.21, -1.19]])\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "                      [np.nan, -0.67, -0,22]])\n",
    "clf = KNeighborsClassfier(3, weights='distance')\n",
    "trained_model = clf.fit(X[:,1:],X[:,0])\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ee3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------- # Chapter9: Dimensionality Reduction Using Feature Extraction --------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "features_pca = pca.fit_transform(features)\n",
    "print(\"Original number of features\", features.shape[1])\n",
    "print(\"Reduced number of features\", features_pca.shape[1])\n",
    "\"\"\" PCA principle: http://www.math.union.edu/~jaureguj/PCA.pdf\"\"\"\n",
    "# Using an extension of PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "feature, _ = make_circles(n_samples=1000, random_state = 1, noise=0.1, factor=0.1)\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kpca.shape[1])\n",
    "\"\"\" https://sebastianraschka.com/Articles/2014_kernel_pca.html \"\"\"\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "# Reducing features by Maximizing class separability\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(features, target).transform(features)\n",
    "lda.explained_variance_ratio_\n",
    "#Choose n_components\n",
    "lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "features_lda = lda.fit(features, target)\n",
    "lda_var_ratios = lda.explained_variance_ratio_\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    total_variance = 0.0\n",
    "    n_components = 0\n",
    "    for explained_variance in var_ratio:\n",
    "        total_variance += explained_variance\n",
    "        n_components += 1\n",
    "        if total_variance >= goal_ratio:\n",
    "            break\n",
    "    return n_components\n",
    "select_n_components(lda_var_ratios, 0.95)\n",
    "# Reducing using Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "nmf = NMF(n_components=10, random_state=1)\n",
    "features_nmf = nmf.fit_transform(features)\n",
    "# Reducing on Parse Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr.matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "digits = datasets.load_digits()\n",
    "features = StandardScaler().fit_transform(digits)\n",
    "features_sparse = csr.matrix(features)\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754646ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------- # Chapter10: Dimensionality Reduction Using feature selection ------------------------\n",
    "# Thresholding Numerical Feature Variance\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "features_high_variance[0:3]\n",
    "# Handling highly correlated features\n",
    "dataframe.corr()\n",
    "---> remove correlated features\n",
    "# Removing Irrelevant features for Classfication\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "features = features.astype(int)\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "# Using SelectPercentile\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "fvalue_selector = SelectPerccentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "# Recursively Eliminating features\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "features, target = make_regression(n_samples=10000, n_features=100, n_informative=2)\n",
    "ols = linear_model.LinearRegression()\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)\n",
    "rfecv.n_features_\n",
    "rfecv.ranking_\n",
    "\"\"\"https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a62992",
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------------- #Chapter11: Model Evaluation -----------------------------------------------\n",
    "# Cross-Validating Models\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticsRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "target = digits.target\n",
    "standardizer = StandardScaler()\n",
    "lr = LogisticRegression()\n",
    "pipeline = make_pipeline(standardizer, lr)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cv_results = cross_val_score(pipeline, features, target, cv=kf, scoring=\"accuracy\" #loss function\n",
    "                            n_jobs=-1 #Use all CPU scores)\n",
    "cv_results.mean()\n",
    "\"\"\"Using StratifiedKFold for imbalance target vector for example, 80% male and 20% female\"\"\"\n",
    "# Evaluating Binary Classifier Thresholds\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklean.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curse, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "features, target = make_classification(n_samples=100000, n_features=10, n_classes=2, n_informative=3, random_state=3)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=1)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(features_train, target_train)\n",
    "target_proba = lr.predict_proba(features_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(target test, target_proba)\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.plot(false_postive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0],[1,0], c=\".7\"), plt.plot([1,1],c=\".7\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\"\"\"https://community.alteryx.com/t5/Data-Science/ROC-Curves-in-Python-and-R/ba-p/138430\"\"\"\n",
    "                             \"\"\"http://gim.unmc.edu/dxtests/roc3.htm\"\"\"\n",
    "#Evaluating Multiclass Classifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "features, target = make_classification(n_samples=10000, n_features=3, n_classes=3, n_informative=3,n_redundant=3 ,random_state=1)\n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, features, target, scoring=\"accuracy\")\n",
    "#Visualizing a Classifier's Performance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "class_names = iris.target_names\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=1)\n",
    "classifier = LigisticRegression()\n",
    "target_predicted = classifier.fit(features_train, target_train).predict(features_test)\n",
    "matrix = confusion_matrix(target_test, target_predicted)\n",
    "dataframe = pd.DataFrame(matrix, index = class_names, columns = class_names)\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()\n",
    "\n",
    "#Evaluating Regression\n",
    "#MSE for regression model\n",
    "\"\"\"MSE >< neg_mse\"\"\"\n",
    "#Coefficient of determination\n",
    "\"\"\"https://en.wikipedia.org/wiki/Coefficient_of_determination\"\"\"\n",
    "cross_value_score(ols, features, target, scoring\"neg_mean_squared_error\"|\"r2\")\n",
    "\n",
    "#Evaluating Cluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import K_Means\n",
    "from sklearn.datasets import make_blobs\n",
    "features, _ = make_blobs(n_samples = 1000, n_features=10, centers=3, cluster_std=.5, shuffle=True, random_state=1)\n",
    "model = KMeans(n_clusters=3, random_state=1).fit(features)\n",
    "target_predicted = model.labels_\n",
    "silhoutte_score(features, target_predicted)\n",
    "\"\"\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7626649",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------- #Chapter12: Model Selection ----------------------------------------------------\n",
    "#Select models using Exhaustive Search\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = dataset.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "lr = linear_model.LogisticRegression()\n",
    "penalty = ['l1','l2']\n",
    "C = np.logspace(0,4,10)\n",
    "hyperparameters = dict(C=C, penalty = penalty)\n",
    "gridsearch = GridSearchCV(lr, hyperparameters, cv=5, verbose=0)\n",
    "best_model = gridsearch.fit(features, target)\n",
    "best_model.best_estimator_.get_params()\n",
    "\n",
    "#Select models using Randomized Search\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "iris = dataset.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "lr = linear_model.LogisticRegression()\n",
    "penalty = ['l1','l2']\n",
    "C = uniform(loc=0, scale=4)\n",
    "hyperparameters = dict(C=C, penalty = penalty)\n",
    "randomizedsearch = RandomizedSearchCV(lr, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "best_model = randomizedsearch.fit(features, target)\n",
    "\"\"\"https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\"\"\"\n",
    "\n",
    "#Select Models from multiple learning algorithms\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(0)\n",
    "iris = dataset.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "pipe = Pipeline([('Classifier',RandomForestClassfier())])\n",
    "search_space = [{\"classifier\": [LogisticRegression()],\n",
    "                 \"classifier_penalty\" : ['l1','l2'],\n",
    "                 \"classifier_C\": np.logspace(0,4,10)},\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_features\": [1, 2, 3]}]\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "best_model = gridsearch.fit(features, target)\n",
    "best_model.best_estimator_.get_params()['classifier']\n",
    "\n",
    "#Select models when processing\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(0)\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "pipe = pipeline([(\"preprocess\", preprocess),\n",
    "                 (\"classifier\", LogisticRegression())])\n",
    "search_space = [{\"preprocess__pca__n_components\": 1,2,3],\n",
    "                 \"classifier_penalty\": ['l1','l2'],\n",
    "                 \"classifier_C\": np.logspace(0,4,10)}]\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "best_model = clf.fit(features, target)\n",
    "\n",
    "#Evaluating Performance after model selection\n",
    "\"\"\"Conduct nested cross-validation and output the average score\"\"\"\n",
    "cross_val_score(gridsearch, features, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b56ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------#Chapter13: Linear Regression ------------------------------------\n",
    "# Fitting a line\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(features, target)\n",
    "model.intercept_\n",
    "model.coef_\n",
    "\n",
    "#Handling interactive effects\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynominalFeatures\n",
    "boston = load_boston()\n",
    "features = boston.data[:, 0:2]\n",
    "target = boston.target\n",
    "interaction = PolynominalFeatures(degree=3, include_bias=False, interaction_only=True)\n",
    "features_interaction = interaction.fit_transform(features)\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(features_interaction, target)\n",
    "\n",
    "#Fitting a Non-Linear Relationship\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynominalFeatures\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:1]\n",
    "target = boston.target\n",
    "polynominal = PolynominalFeatures(degress=3, include_bias=False)\n",
    "features_polynominal = polynominal.fit_transform(features)\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(features_polynominal, target)\n",
    "\n",
    "#Reduce Variance with Regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target\n",
    "scaler = StandardScaler()\n",
    "features_standardizer = scaler.fit_transform(features)\n",
    "lr = Ridge(alpha=0.5)\n",
    "model = lr.fit(features_standardizer, target)\n",
    "\"\"\"select alpha\"\"\"\n",
    "from sklearn.linear_model import RidgeCV\n",
    "regr_cv = RidgeCV(aphas=[0.1, 1.0, 10.0])\n",
    "model_cv = regr_cv.fit(features_standardized, target)\n",
    "model_cv.alpha_\n",
    "\n",
    "#Reduce Features with Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "lr = Lasso(alpha=0.5)\n",
    "model = lr.fit(features_standardized, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac14726",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------ #Chapter14: Trees and Forests --------------------------------------------\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from sklearn import tree\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "model = dt.fit(features, target)\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, feature_names = iris.feature_names, class_names = iris.target_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "#DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn fimport datasets\n",
    "boston = datasets.load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "dt = DecisionTreeRegressor(criterion='mae', random_state=0)\n",
    "model = dt.fit(features, target)\n",
    "\n",
    "#RandomForestClassfier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "rf = RandomForestClassifier(criterion=\"entropy\",random_state=0, n_jobs=-1)\n",
    "model = rf.fit(features, target)\n",
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "feature = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "model = rf.fit(features, target)\n",
    "\n",
    "# identify important Features in Random Forests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "model = rf.fit(features, target)\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = [iris.feature_names[i] for i in indices]\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(features.shape[1]), importances[indices])\n",
    "plt.xtick(range(features.shape[1]), names, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#Select important features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "selector = SelectFromModel(rf, threshold=0.3)\n",
    "features_important = selector.fit_transform(features, target)\n",
    "model = rf.fit(features_important, target)\n",
    "\n",
    "#Improve performance through boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "adaboost = AdaBoostClassifier(random_state=0)\n",
    "#Evaluate RandomForest with out-of-bag Errors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "rf = RandomForestClassifier(random_state=0, n_estimators=1000, oob_score=True, n_jobs=-1)\n",
    "model = rf.fit(features, target)\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------- #Chapter15: K-Nearest Neighbors ---------------------------------------\n",
    "#Observation's Nearest Neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "standardizer = StandardScaler()\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
    "\n",
    "#K-Nearest Neighbor Classifier\n",
    "from sklearn.neighbors import KneighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "standardizer = StandardScaler()\n",
    "X_std = standardizer.fit_transform(X)\n",
    "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1).fit(X_std,y)\n",
    "#Identify best neighborhood size\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "fron sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "standardizer = StandardScaler()\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "pipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n",
    "search_space = [{\"knn__n_neighbors\":[1,2,3,4,5,6,7,8,9,10]}]\n",
    "classifier = GridSearchCV(pipe, search_space, cv=5, verbose=0).fit(features_standardized, target)\n",
    "classifier.best_estimator_.get_params()[\"knn__n_neighbors\"]\n",
    "\n",
    "#Radius-Based Nearest Neighbor Classifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "standardizer = StandardScaler()\n",
    "features_standardized = StandardScaler.fit_transform(features)\n",
    "rnn = RadiusNeighborsClassfier(radius=.5, n_jobs=-1).fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------------#Chapter16: LogisticRegression --------------------------------------------\n",
    "#Binary Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "lr = LogisticRegression(random_state=0)\n",
    "model = lr.fit(features_standardized, target)\n",
    "\n",
    "#Multiclass Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "lr = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "model = lr.fit(features_standardized, target)\n",
    "\n",
    "#Reduce Variance Through Regularization\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "lr = LogisticRegressionCV(penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
    "model = lr.fit(features_standardized, target)\n",
    "#Classifier on very large data\n",
    "lr = LogisticRegression(random_state=0, solver=\"sag\")\n",
    "\"\"\"handling imbalance class\"\"\"\n",
    "lr = LogisticRegression(random_state=0, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------#Chapter17: Support Vector Machines -----------------------------------------------\n",
    "#Linear Classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import datasets\n",
    "from sklaern.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100, :2]\n",
    "target = iris.target[:100]\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "svc = LinearSVC(C=1.0)\n",
    "model = svc.fit(features_standardized, target)\n",
    "color = [\"black\" if c==0 else \"lightgrey\" for c in target]\n",
    "plt.scatter(features_standardized[:,0], features_standardized[:,1], c=color)\n",
    "w = svc.coef_[0]\n",
    "a = -w[0]/w[1]\n",
    "xx = np.linspace(-2.5, 2.5)\n",
    "yy = a*xx - (svc.intercept_[0])/w[1]\n",
    "plt.plot(xx,yy)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Handle Linearly Inseparable Classes using Kernels\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "features = np.random.randn(200, 2)\n",
    "target_xor = np.logical_xor(features[:,0] > 0, features[:,1] > 0)\n",
    "target = np.where(target_xor, 0, 1)\n",
    "svc = SVC(kernel=\"rbf\", random_state=0, gamma=1, C=1)\n",
    "model = svc.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2baeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------#Chapter18: Naive Bayes -----------------------------------------------\n",
    "#Training a Classifier for continous features\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "classifier = GaussianNB()\n",
    "model = classifier.fit(features, target)\n",
    "\"\"\"Gaussian naive bayes is best used in cases all our features are continuous\"\"\"\n",
    "\n",
    "#Training a Classifier for Discrete and count features\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = np.array(['I love VN. VN!',\n",
    "                     'VN is best',\n",
    "                     'Brazil beats both'])\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "features = bag_of_words.toarray()\n",
    "target = np.array([0,0,1])\n",
    "classifier = MultinomialNB(class_prior = [0.25,0.5])\n",
    "model = classifier.fit(features, target)\n",
    "\"\"\"Most common uses of multinomial naive Bayes is text classification\"\"\"\n",
    "\n",
    "#Training a Naive Bayes Classifier for Binary Features\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "features = np.random.randint(2, size=(100,3))\n",
    "target = np.random.radint(2, size=(100,1)).ravel()\n",
    "classifier = BernoulliNB(class_prior=[0.25,0.5])\n",
    "model = classifier.fit(features, target)\n",
    "\"\"\"The bernoulli naive bayes classifer assumes that all features are binary\"\"\"\n",
    "\n",
    "#Calibrating predicted probabilities\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "classifier = GaussianNB()\n",
    "classifier_sigmoid = CalibratedClassifierCV(classifier, cv=2, method='sigmoid')\n",
    "classifier_sigmoid.fit(features, target)\n",
    "new_observation = [[2.6, 2.6, 2.6, 0.4]]\n",
    "classifier_sigmoid.predict_proba(new_observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------#Chapter19: Clustering ------------------------------------------------\n",
    "#Cluster using K-Means\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "cluster = KMeans(n_clusters=3, random_state=0, n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "model.cluster_center_\n",
    "\n",
    "#Cluster using Meanshift\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MeanShift\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = standardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "cluster = MeanShift(n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "#Cluster using DBSCAN\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "cluster = DBSCAN(n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "#Cluster using Hierarchical merging\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "cluster = AgglomerativeClustering(n_clusters=3)\n",
    "model = cluster.fit(features_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------#Chapter20: Neural Networks ------------------------------------------------\n",
    "#Preprocessing Data for Neural Network\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "#Designing a Neural Network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(lyaers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\"\"\"Binary classification: one unit with a sigmoid activation function and Binary cross-entropy\"\"\"\n",
    "\"\"\"Multiclass classification: k(number of target classes) units with a softmax activation function and Categorical cross-entropy\"\"\"\n",
    "\"\"\"Regression: one unit  with no activation function and Mean squared error\"\"\"\n",
    "\n",
    "#Training a Binary Classifier\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "tokenzier = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.squences_to_matrix(data_test, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=1, batch_size=100, validation_data=(features_test, target_test))\n",
    "predicted_target = network.predict(features_test)\n",
    "\n",
    "#Training a Multiclass Classifier\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(0)\n",
    "number_of_features = 5000\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=100, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "network.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=0, batch_size=100, validation_data=(features_test, target_test))\n",
    "predicted_target = network.predict(features_test)\n",
    "\n",
    "#Training a Regressor\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(0)\n",
    "features, target = make_regression(n_samples=10000, n_features=3, n_informative=3, n_targets=1, noise=0.0, random_state=0)\n",
    "features_train, features_target, target_train, target_test = train_test_split(features, target, test_size=0.33, random_state=0)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=32, activation=\"relu\", input_shape=(features_train.shape[1],)))\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1))\n",
    "network.compile(loss=\"mse\", optimizer=\"RMSprop\", metrics=['mse'])\n",
    "history = network.fit(features_train, target_train, epochs=10, verbose=0, batch_size=100, validation_data=(features_test, target_test))\n",
    "predicted_target = network.predict(features_test)\n",
    "\n",
    "#Visualize Training History\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pylot as plt\n",
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "tokenzier = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.squences_to_matrix(data_test, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "history = network.fit(features_train, target_train, epochs=15, verbose=0, batch_size=1000, validation_data=(features_test, target_test))\n",
    "\n",
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history[\"acc\"]\n",
    "test_accuracy = history.history[\"val_acc\"]\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "#Reducing Overfitting with Weight Regularization\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", kernel_regularizer=regularizer.l2(0.01), input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", kernel_regularizer=relularizer.l2(0.01)))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=0, batch_size=100, validation_data=(features_test, target_test))\n",
    "\n",
    "#Reducing Overfitting with Eearly Stop\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\")\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping (monitor=\"val_loss\", pattience=2), ModelCheckpoint(filepath=\"best_model.h5\", monitor=\"val_loss\", save_best_only=True)]\n",
    "history = network.fit(features_train, target_train, epochs=20, callbacks=callbacks,verbose=0, batch_size=100, validation_data=(features_test, target_test))\n",
    "\n",
    "#Reducing Overfitting with Dropout\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\")\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "history = network.fit(features_train, target_train, epochs=3, callbacks=callbacks,verbose=0, batch_size=100, validation_data=(features_test, target_test))\n",
    "\n",
    "#K-Fold Validating Neural Networks\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets imports make_classification\n",
    "np.random.seed(0)\n",
    "number_of_features=100\n",
    "features, target = make_classification(n_samples=10000, n_features=number_of_features, n_informative=3, n_redundant=0, n_classes=2, weights=[.5,.5], random_state=0)\n",
    "def create_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "    return network\n",
    "neural_network = KerasClassifier(build_fn=create_network, epochs=10, batch_size=100, verbose=0)\n",
    "cross_val_score(neural_network, features, target, cv=3)\n",
    "\n",
    "#Tuning Neural Network\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassfier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "np.random.seed(0)\n",
    "number_of_features = 100\n",
    "features, target = make_classification(n_samples=10000, n_features=number_of_features, n_informative=3, n_redundant=0, n_classes=2, weights=[.5, .5], random_state=0)\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    network.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return network\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
    "epochs = [5,10]\n",
    "batches = [5,10,100]\n",
    "optimizers = [\"rmsprop\",\"adam\"]\n",
    "hyperparameters = dict(optimizers=optimizers, epochs=epochs, batch_size=batches)\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "grid_result = grid.fit(features, target)\n",
    "grid_result.best_param_\n",
    "            \n",
    "#Visualizing Neural Network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from Ipython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "SVG(model_to_dot(network, show_shapes=True).create(prog=\"dot\", format=\"svg\"))\n",
    "plot_model(network, show_shapes=True, to_file=\"network.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e3c948",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-34dd6fb9c35d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Classifying Images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Classifying Images\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "np.random.seed(0)\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "(data_train, data_test), (target_train, target_test) = mnist.load_data()\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "target_train = np.utils.to_categorical(target_train)\n",
    "target_test = np.utils.to_categorical(targat_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "network = Sequential()\n",
    "network.add(Conv2D(filters=64, kernel_size=(5,5), input_shape=(channels, width, height), activation=\"relu\"))\n",
    "network.add(MaxPooling2D(pool_size=(2,2)))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "network.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "network.fit(features_train, target_train, epochs=2, verbose=0, batch_size=1000, validation_data=(feature_test, target_test))\n",
    "\"\"\"Convolutional neural network are popular type of network effect at computer vision \"\"\"\n",
    "\n",
    "\n",
    "#Improving Performance with Image Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, zoom_range=0.3, width_shift_range=0.2, horizontal_flip=True, rotation_range=90)\n",
    "augment_images = augmentation.flow_from_diretory(\"raw/images\", batch_size=32, class_mode=\"binary\", save_to_dir=\"processed/images\")\n",
    "network.fit_generator(augment_images, steps_per_epoch=2000, epochs=5, validation_data=augment_images_test, validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying Text\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
    "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "network.add(layers.LSTM(units=128))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=['accuracy'])\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=0, batch_size=1000, validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d709a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------ #Chapter21: Saving and Loding Trained Model ---------------------------------------------\n",
    "#Saving & Loading scikit-learn model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.externals import joblib\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "classifier = RandomForestClassifier()\n",
    "model = classifier.fit(features, target)\n",
    "joblib.dump(model, \"model.pkl\")\n",
    "classifier = joblib.load(\"model.pkl\")\n",
    "\n",
    "#Saving & Loading a Keras Model\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words = number_of_features)\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "train_features = tokenizer.sequences_to_matrix(train_data, mode=\"binary\")\n",
    "test_features = tokenizer.sequences_to_matrix(test_data, mode=\"binary\")\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\",input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "history = network.fit(train_features, train_target, epochs=3, verbose=0, batch_size=100, validation_data=(test_features, test_target))\n",
    "network.save(\"model.h5\")\n",
    "network = load_model(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
